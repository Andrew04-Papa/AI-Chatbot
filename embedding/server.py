import torch
from sentence_transformers import SentenceTransformer, util
import fitz
from docx import Document
import pandas as pd
from flask import Flask, request, jsonify, session
from flask import send_from_directory
from flask_cors import CORS
import psycopg2
import psycopg2.extras
import sys
import os
import json
import unicodedata
import uuid
from datetime import datetime, timedelta
import re
import threading

app = Flask(__name__)
app.secret_key = 'your-secret-key-here-change-this'
CORS(app, supports_credentials=True)

# Lazy loading cho model ƒë·ªÉ tƒÉng t·ªëc startup
model = None
qa_embeddings = torch.tensor([])
qa_corpus = []

def load_model():
    """Lazy load sentence transformer model"""
    global model
    if model is None:
        print("üîÑ ƒêang t·∫£i AI model...")
        model = SentenceTransformer("all-MiniLM-L6-v2")
        print("‚úÖ ƒê√£ t·∫£i AI model th√†nh c√¥ng!")
    return model

# Th√™m ƒë∆∞·ªùng d·∫´n server v√†o sys.path ƒë·ªÉ import db_config
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'server'))

try:
    from db_config import DB_CONFIG
    print("‚úÖ ƒê√£ load DB config th√†nh c√¥ng")
except ImportError as e:
    print(f"‚ùå L·ªói import DB config: {e}")
    DB_CONFIG = {
        'host': 'localhost',
        'database': 'AI_Chatbot',
        'user': 'postgres',
        'password': 'hoangduy2k4',
        'port': 5432
    }

def get_db_connection():
    """T·∫°o k·∫øt n·ªëi ƒë·∫øn PostgreSQL database v·ªõi error handling"""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        conn.autocommit = True
        return conn
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi database: {e}")
        return None

def get_or_create_session_id():
    """L·∫•y ho·∫∑c t·∫°o session ID m·ªõi"""
    if 'session_id' not in session:
        session['session_id'] = str(uuid.uuid4())
    return session['session_id']

def format_text_response(text):
    """Format text ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp h∆°n v·ªõi line breaks"""
    if not text:
        return text
    
    # Thay th·∫ø c√°c k√Ω t·ª± xu·ªëng d√≤ng ƒë·∫∑c bi·ªát
    text = text.replace('\\r\\n', '\n').replace('\\n', '\n').replace('\r\n', '\n')
    
    # Th√™m xu·ªëng d√≤ng sau d·∫•u ch·∫•m n·∫øu c√¢u qu√° d√†i
    sentences = text.split('. ')
    formatted_sentences = []
    
    for i, sentence in enumerate(sentences):
        sentence = sentence.strip()
        if sentence:
            # Th√™m d·∫•u ch·∫•m l·∫°i n·∫øu kh√¥ng ph·∫£i c√¢u cu·ªëi
            if i < len(sentences) - 1 and not sentence.endswith('.'):
                sentence += '.'
            
            # Xu·ªëng d√≤ng sau m·ªói c√¢u d√†i h∆°n 100 k√Ω t·ª±
            if len(sentence) > 100:
                sentence += '\n'
            
            formatted_sentences.append(sentence)
    
    result = ' '.join(formatted_sentences)
    
    # Th√™m xu·ªëng d√≤ng tr∆∞·ªõc c√°c bullet points
    result = re.sub(r'([.!?])\s*([‚Ä¢\-\*])', r'\1\n\2', result)
    
    # Th√™m xu·ªëng d√≤ng tr∆∞·ªõc c√°c ti√™u ƒë·ªÅ (ch·ªØ in hoa)
    result = re.sub(r'([.!?])\s*([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê]{3,})', r'\1\n\n\2', result)
    
    # Th√™m xu·ªëng d√≤ng tr∆∞·ªõc th√¥ng tin li√™n h·ªá
    result = re.sub(r'([.!?])\s*(üìû|‚òéÔ∏è|üìß|üåê|ƒê·ªãa ch·ªâ|Email|Website|ƒêi·ªán tho·∫°i)', r'\1\n\n\2', result)
    
    return result

def format_file_content(content, filename):
    """Format n·ªôi dung file ƒë·ªÉ hi·ªÉn th·ªã g·ªçn g√†ng v√† ƒë·∫πp"""
    if not content:
        return create_file_content_html("File tr·ªëng ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c.", filename)
    
    # L√†m s·∫°ch content
    content = content.strip()
    
    # T√°ch th√†nh c√°c sections d·ª±a tr√™n "---" ho·∫∑c "Trang"
    formatted_content = ""
    
    if "--- Trang" in content:
        pages = content.split("--- Trang")
        for i, page in enumerate(pages):
            if page.strip():
                if i > 0:
                    formatted_content += f'<div class="page-separator">üìÑ Trang {i}</div>'
                formatted_content += format_text_for_file(page.strip())
    else:
        # Format text th√¥ng th∆∞·ªùng
        formatted_content = format_text_for_file(content)
    
    return create_file_content_html(formatted_content, filename)

def format_text_for_file(text):
    """Format text cho file content - g·ªçn g√†ng h∆°n"""
    if not text:
        return text
    
    # Thay th·∫ø c√°c k√Ω t·ª± xu·ªëng d√≤ng ƒë·∫∑c bi·ªát
    text = text.replace('\\r\\n', '\n').replace('\\n', '\n').replace('\r\n', '\n')
    
    # L√†m s·∫°ch text - lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    
    # Gh√©p l·∫°i v·ªõi xu·ªëng d√≤ng ƒë∆°n
    result = '\n'.join(lines)
    
    # Th√™m xu·ªëng d√≤ng tr∆∞·ªõc c√°c bullet points
    result = re.sub(r'([.!?])\s*([‚Ä¢\-\*])', r'\1\n\2', result)
    
    # Th√™m xu·ªëng d√≤ng tr∆∞·ªõc c√°c ti√™u ƒë·ªÅ quan tr·ªçng
    result = re.sub(r'([.!?])\s*([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê\s]{10,})', r'\1\n\n\2', result)
    
    return result

def create_file_content_html(content, filename):
    """T·∫°o HTML structure cho file content"""
    return f'''<div class="file-content-wrapper">
    <div class="file-content-header">
        <span class="file-icon">üìÑ</span>
        <span>N·ªôi dung: {filename}</span>
    </div>
    <div class="file-content-body" style="padding-top: 0; margin-top: 0;">{content}</div>
    <div class="file-content-footer">Tr√≠ch t·ª´ file: {filename}</div>
</div>'''

def create_file_attachment_html(filename):
    """T·∫°o HTML cho file attachment si√™u g·ªçn g√†ng - ch·ªØ to h∆°n, kho·∫£ng c√°ch g·∫ßn h∆°n"""
    return f'''<div style="display: flex; flex-direction: column; align-items: flex-start;">
    <div style="display: flex; align-items: center; gap: 4px;">
        <span style="font-size: 20px; color: #6c757d;">üìÑ</span>
        <span style="font-weight: bold; font-size: 14px; color: #495057;">T·ªáp ƒë√≠nh k√®m</span>
    </div>
    <a href="http://localhost:5000/files/{filename}" target="_blank" style="color: #7a1ea1; text-decoration: none; font-size: 15px; font-weight: 500; word-break: break-all; line-height: 1.1; margin-top: -2px;" title="{filename}">
        {filename}
    </a>
</div>'''

def save_conversation(session_id, user_message, bot_response, file_referenced=None, response_type='text'):
    """L∆∞u cu·ªôc tr√≤ chuy·ªán v√†o database"""
    conn = get_db_connection()
    if not conn:
        return False
        
    cursor = conn.cursor()
    try:
        cursor.execute("""
            INSERT INTO conversations (session_id, user_message, bot_response, file_referenced, response_type)
            VALUES (%s, %s, %s, %s, %s)
        """, (session_id, user_message, bot_response, file_referenced, response_type))
        conn.commit()
        return True
    except Exception as e:
        print(f"L·ªói khi l∆∞u conversation: {e}")
        return False
    finally:
        cursor.close()
        conn.close()

def get_conversation_history(session_id, limit=10):
    """L·∫•y l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán"""
    conn = get_db_connection()
    if not conn:
        return []
        
    cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
    try:
        cursor.execute("""
            SELECT user_message, bot_response, file_referenced, response_type, created_at
            FROM conversations 
            WHERE session_id = %s 
            ORDER BY created_at DESC 
            LIMIT %s
        """, (session_id, limit))
        rows = cursor.fetchall()
        return list(reversed(rows))
    except Exception as e:
        print(f"L·ªói khi l·∫•y conversation history: {e}")
        return []
    finally:
        cursor.close()
        conn.close()

def get_last_referenced_file(session_id):
    """L·∫•y file ƒë∆∞·ª£c tham chi·∫øu g·∫ßn nh·∫•t trong cu·ªôc tr√≤ chuy·ªán"""
    conn = get_db_connection()
    if not conn:
        return None
        
    cursor = conn.cursor()
    try:
        cursor.execute("""
            SELECT file_referenced 
            FROM conversations 
            WHERE session_id = %s AND file_referenced IS NOT NULL 
            ORDER BY created_at DESC 
            LIMIT 1
        """, (session_id,))
        result = cursor.fetchone()
        return result[0] if result else None
    except Exception as e:
        print(f"L·ªói khi l·∫•y last referenced file: {e}")
        return None
    finally:
        cursor.close()
        conn.close()

def get_file_content(filename):
    """L·∫•y n·ªôi dung file t·ª´ file system"""
    upload_folder = os.path.join(os.path.dirname(__file__), "uploads")
    file_path = os.path.join(upload_folder, filename)
    ext = os.path.splitext(filename)[1].lower()
    
    try:
        if ext == ".pdf":
            doc = fitz.open(file_path)
            text = ""
            for page_num, page in enumerate(doc):
                page_text = page.get_text()
                text += f"\n--- Trang {page_num + 1} ---\n{page_text}"
            return text
        elif ext == ".docx":
            doc = Document(file_path)
            text = "\n".join([p.text for p in doc.paragraphs])
            return text
        elif ext == ".txt":
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
            return text
        else:
            return "Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng file n√†y"
    except Exception as e:
        return f"L·ªói khi ƒë·ªçc file: {str(e)}"

def load_qa():
    """Load Q&A t·ª´ database v·ªõi error handling"""
    conn = get_db_connection()
    if not conn:
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi database ƒë·ªÉ load Q&A")
        return []
        
    cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
    try:
        cursor.execute("SELECT id, questions, answer, answer_file FROM qa_entries ORDER BY id")
        rows = cursor.fetchall()
        qa_data = []
        for row in rows:
            questions = row['questions']
            if isinstance(questions, str):
                try:
                    questions = json.loads(questions)
                except Exception:
                    questions = [questions]
            qa_data.append({
                'id': row['id'],
                'questions': questions,
                'answer': row['answer'],
                'answer_file': row['answer_file']
            })
        print(f"‚úÖ ƒê√£ load {len(qa_data)} Q&A entries")
        return qa_data
    except Exception as e:
        print(f"‚ùå L·ªói load Q&A: {e}")
        return []
    finally:
        cursor.close()
        conn.close()

def update_qa_embeddings():
    """C·∫≠p nh·∫≠t embeddings v·ªõi lazy loading"""
    global qa_embeddings, qa_corpus
    try:
        qa_list = load_qa()
        qa_corpus = [q for qa in qa_list for q in qa["questions"]]
        if qa_corpus:
            # Lazy load model khi c·∫ßn
            model = load_model()
            qa_embeddings = model.encode(qa_corpus, convert_to_tensor=True)
            print(f"‚úÖ ƒê√£ t·∫°o embeddings cho {len(qa_corpus)} c√¢u h·ªèi")
        else:
            qa_embeddings = torch.tensor([])
            print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o embeddings")
    except Exception as e:
        print(f"‚ùå L·ªói t·∫°o embeddings: {e}")
        qa_embeddings = torch.tensor([])

def normalize_text(text):
    """Chu·∫©n h√≥a text"""
    text = unicodedata.normalize('NFD', text)
    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])
    return text.lower().strip()

def clear_conversation_history(session_id):
    """X√≥a l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán"""
    conn = get_db_connection()
    if not conn:
        return False
        
    cursor = conn.cursor()
    try:
        cursor.execute("DELETE FROM conversations WHERE session_id = %s", (session_id,))
        conn.commit()
        return True
    except Exception as e:
        print(f"L·ªói khi x√≥a conversation history: {e}")
        return False
    finally:
        cursor.close()
        conn.close()

# API Routes
@app.route("/api/qa", methods=["GET"])
def api_get_qa():
    data = load_qa()
    return jsonify(data)

@app.route("/qa-list", methods=["GET"])
def qa_list():
    data = load_qa()
    return jsonify(data)

@app.route("/qa-add", methods=["POST"])
def qa_add():
    questions = request.form.getlist("questions[]")
    if not questions or (len(questions) == 1 and not questions[0].strip()):
        return jsonify({"message": "C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng!"}), 400
    if isinstance(questions, str):
        questions = [questions]
    answer = request.form.get("answer", "")
    file = request.files.get("file")
    answer_file = None

    if file:
        upload_folder = os.path.join(os.path.dirname(__file__), "uploads")
        os.makedirs(upload_folder, exist_ok=True)
        file.save(os.path.join(upload_folder, file.filename))
        answer_file = file.filename

    conn = get_db_connection()
    if not conn:
        return jsonify({"message": "L·ªói k·∫øt n·ªëi database"}), 500
        
    cursor = conn.cursor()
    try:
        cursor.execute(
            "INSERT INTO qa_entries (questions, answer, answer_file) VALUES (%s, %s, %s)",
            (json.dumps(questions, ensure_ascii=False), answer, answer_file)
        )
        conn.commit()
        # Async update embeddings ƒë·ªÉ kh√¥ng block request
        threading.Thread(target=update_qa_embeddings).start()
        return jsonify({"message": "success"})
    except Exception as e:
        print(f"L·ªói th√™m Q&A: {e}")
        return jsonify({"message": "L·ªói th√™m Q&A"}), 500
    finally:
        cursor.close()
        conn.close()

@app.route("/qa-update/<int:index>", methods=["POST"])
def qa_update(index):
    questions = request.form.getlist("questions[]")
    if not questions or (len(questions) == 1 and not questions[0].strip()):
        return jsonify({"message": "C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng!"}), 400
    if isinstance(questions, str):
        questions = [questions]
    answer = request.form.get("answer", "")
    file = request.files.get("file")
    answer_file = None

    if file:
        upload_folder = os.path.join(os.path.dirname(__file__), "uploads")
        os.makedirs(upload_folder, exist_ok=True)
        file.save(os.path.join(upload_folder, file.filename))
        answer_file = file.filename

    conn = get_db_connection()
    if not conn:
        return jsonify({"message": "L·ªói k·∫øt n·ªëi database"}), 500
        
    cursor = conn.cursor()
    try:
        if answer_file:
            cursor.execute(
                "UPDATE qa_entries SET questions=%s, answer=%s, answer_file=%s WHERE id=%s",
                (json.dumps(questions, ensure_ascii=False), answer, answer_file, index+1)
            )
        else:
            cursor.execute(
                "UPDATE qa_entries SET questions=%s, answer=%s WHERE id=%s",
                (json.dumps(questions, ensure_ascii=False), answer, index+1)
            )
        conn.commit()
        threading.Thread(target=update_qa_embeddings).start()
        return jsonify({"message": "success"})
    except Exception as e:
        print(f"L·ªói c·∫≠p nh·∫≠t Q&A: {e}")
        return jsonify({"message": "L·ªói c·∫≠p nh·∫≠t Q&A"}), 500
    finally:
        cursor.close()
        conn.close()

@app.route("/qa-delete/<int:index>", methods=["DELETE"])
def qa_delete(index):
    conn = get_db_connection()
    if not conn:
        return jsonify({"message": "L·ªói k·∫øt n·ªëi database"}), 500
        
    cursor = conn.cursor()
    try:
        cursor.execute("DELETE FROM qa_entries WHERE id=%s", (index+1,))
        conn.commit()
        threading.Thread(target=update_qa_embeddings).start()
        return jsonify({"message": "deleted"})
    except Exception as e:
        print(f"L·ªói x√≥a Q&A: {e}")
        return jsonify({"message": "L·ªói x√≥a Q&A"}), 500
    finally:
        cursor.close()
        conn.close()

@app.route("/files/<filename>")
def serve_file(filename):
    upload_folder = os.path.join(os.path.dirname(__file__), "uploads")
    return send_from_directory(upload_folder, filename, as_attachment=True)

@app.route("/conversation-history")
def get_conversation_history_api():
    """API ƒë·ªÉ l·∫•y l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán"""
    session_id = get_or_create_session_id()
    history = get_conversation_history(session_id)
    return jsonify({"session_id": session_id, "history": history})

@app.route("/clear-chat", methods=["POST"])
def clear_chat():
    """API ƒë·ªÉ x√≥a l·ªãch s·ª≠ chat"""
    session_id = get_or_create_session_id()
    success = clear_conversation_history(session_id)
    if success:
        return jsonify({"message": "success"})
    else:
        return jsonify({"message": "error"}), 500

@app.route("/chat", methods=["POST"])
def chat():
    data = request.get_json()
    question = data.get("question") or data.get("message") or ""
    session_id = get_or_create_session_id()
    
    question_norm = normalize_text(question)
    qa_list = load_qa()
    
    print(f"DEBUG: Session ID: {session_id}")
    print(f"DEBUG: C√¢u h·ªèi: {question}")
    
    # Ki·ªÉm tra xem ng∆∞·ªùi d√πng c√≥ h·ªèi v·ªÅ n·ªôi dung file kh√¥ng
    content_keywords = ["n·ªôi dung", "b√™n trong", "trong file", "chi ti·∫øt", "ƒë·ªçc file", "xem file", "file c√≥ g√¨"]
    is_asking_content = any(keyword in question_norm for keyword in content_keywords)
    
    if is_asking_content:
        last_file = get_last_referenced_file(session_id)
        if last_file:
            print(f"DEBUG: Tr·∫£ v·ªÅ n·ªôi dung file: {last_file}")
            file_content = get_file_content(last_file)
            formatted_response = format_file_content(file_content, last_file)
            
            save_conversation(session_id, question, formatted_response, last_file, 'file_content')
            return jsonify({"answer": formatted_response})
        else:
            response = "‚ùå B·∫°n ch∆∞a h·ªèi v·ªÅ file n√†o c·∫£. H√£y h·ªèi v·ªÅ m·ªôt file tr∆∞·ªõc, sau ƒë√≥ t√¥i s·∫Ω c√≥ th·ªÉ cho b·∫°n xem n·ªôi dung b√™n trong."
            save_conversation(session_id, question, response, None, 'text')
            return jsonify({"answer": response})
    
    # X·ª≠ l√Ω Q&A b√¨nh th∆∞·ªùng
    # So kh·ªõp tuy·ªát ƒë·ªëi tr∆∞·ªõc
    for qa in qa_list:
        for q in qa["questions"]:
            q_norm = normalize_text(q)
            if question_norm == q_norm:
                if qa.get("answer_file"):
                    file_html = create_file_attachment_html(qa["answer_file"])
                    save_conversation(session_id, question, "File ƒë∆∞·ª£c tham chi·∫øu", qa["answer_file"], 'file')
                    return jsonify({"answer": file_html})
                else:
                    response = format_text_response(qa.get("answer", "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi"))
                    save_conversation(session_id, question, response, None, 'text')
                    return jsonify({"answer": response})

    # So kh·ªõp g·∫ßn ƒë√∫ng
    if len(question_norm) > 4:
        for qa in qa_list:
            for q in qa["questions"]:
                q_norm = normalize_text(q)
                if question_norm in q_norm or q_norm in question_norm:
                    if qa.get("answer_file"):
                        file_html = create_file_attachment_html(qa["answer_file"])
                        save_conversation(session_id, question, "File ƒë∆∞·ª£c tham chi·∫øu", qa["answer_file"], 'file')
                        return jsonify({"answer": file_html})
                    else:
                        response = format_text_response(qa.get("answer", "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi"))
                        save_conversation(session_id, question, response, None, 'text')
                        return jsonify({"answer": response})

    # N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y, d√πng embedding
    if len(qa_embeddings) > 0:
        try:
            model = load_model()  # Lazy load model
            query_vec = model.encode([question], convert_to_tensor=True)
            cos = util.cos_sim(query_vec, qa_embeddings)[0]
            idx = torch.argmax(cos).item()
            score = cos[idx].item()
            print(f"DEBUG: ƒêi·ªÉm embedding: {score}")
            
            if score > 0.5:
                matched_q = qa_corpus[idx]
                for qa in qa_list:
                    if matched_q in qa["questions"]:
                        if qa.get("answer_file"):
                            file_html = create_file_attachment_html(qa["answer_file"])
                            save_conversation(session_id, question, "File ƒë∆∞·ª£c tham chi·∫øu", qa["answer_file"], 'file')
                            return jsonify({"answer": file_html})
                        else:
                            response = format_text_response(qa.get("answer", "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi"))
                            save_conversation(session_id, question, response, None, 'text')
                            return jsonify({"answer": response})
        except Exception as e:
            print(f"L·ªói embedding search: {e}")

    # Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi
    response = "‚ùå Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p."
    save_conversation(session_id, question, response, None, 'text')
    return jsonify({"answer": response})

@app.route("/file-content/<filename>")
def file_content(filename):
    """API ƒë·ªÉ l·∫•y n·ªôi dung file"""
    try:
        content = get_file_content(filename)
        formatted_content = format_file_content(content, filename)
        return jsonify({"content": formatted_content})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Kh·ªüi t·∫°o nhanh - kh√¥ng load model ngay
print("üöÄ Server ƒë√£ s·∫µn s√†ng! (AI model s·∫Ω ƒë∆∞·ª£c t·∫£i khi c·∫ßn)")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
